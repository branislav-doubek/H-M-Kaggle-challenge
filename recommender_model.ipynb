{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ1C0j6bM8Ns"
      },
      "source": [
        "Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM3uchF8deZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b04184-20dd-45d9-8a96-c209de460a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 85 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 36.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfpLMl1tc4OE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, Text\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "02mrDtoFL007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e396c0-7ad3-4654-b6af-664579ef4c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles_ds = tf.data.experimental.make_csv_dataset(\n",
        "    './drive/MyDrive/csv_data/articles.csv',\n",
        "    batch_size=1, # Artificially small to make examples easier to show.\n",
        "    ignore_errors=True,)"
      ],
      "metadata": {
        "id": "V_ocE3Dmmsot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_ds = articles_ds.map(lambda x: { \"article_id\": tf.squeeze(x[\"article_id\"])})"
      ],
      "metadata": {
        "id": "JxPz8QH8oUZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_ds"
      ],
      "metadata": {
        "id": "y0-JwRcN3rss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc276388-e92d-42ff-f154-cbac8e7b7833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec={'article_id': TensorSpec(shape=(), dtype=tf.int32, name=None)}>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ2aLaCndSYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1667d4f1-e178-41ac-caa8-58b10bd9f6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'active': array([1]),\n",
            " 'age': array([19]),\n",
            " 'article_id': array([b'737260001', b'793108003', b'809487001', b'0', b'0'], dtype=object),\n",
            " 'club_member_status': array([b'ACTIVE'], dtype=object),\n",
            " 'colour_group_code': array([ 9, 43,  9,  0,  0]),\n",
            " 'department_no': array([1515, 1338, 1338,    0,    0]),\n",
            " 'fashion_news_frequency': array([b'Regularly'], dtype=object),\n",
            " 'fn': array([1]),\n",
            " 'garment_group_no': array([1010, 1017, 1017,    0,    0]),\n",
            " 'graphical_appearance_no': array([1010016, 1010016, 1010016,       0,       0]),\n",
            " 'index_code': array([b'A', b'B', b'B', b'UNK', b'UNK'], dtype=object),\n",
            " 'index_group_no': array([1, 1, 1, 0, 0]),\n",
            " 'label_article_id': array([b'874961004'], dtype=object),\n",
            " 'perceived_colour_master_id': array([ 5, 18,  5,  0,  0]),\n",
            " 'perceived_colour_value_id': array([4, 4, 4, 0, 0]),\n",
            " 'price': array([0.00337288, 0.0169322 , 0.03049153, 0.        , 0.        ],\n",
            "      dtype=float32),\n",
            " 'product_code': array([737260, 793108, 809487,      0,      0]),\n",
            " 'product_type_no': array([253, 306, 283,   0,   0]),\n",
            " 'sales_channel_id': array([1, 1, 1, 0, 0]),\n",
            " 'section_no': array([11, 61, 61,  0,  0]),\n",
            " 'timestamp': array([1595376000000000000, 1595376000000000000, 1595376000000000000,\n",
            "                         0,                   0])}\n"
          ]
        }
      ],
      "source": [
        "train_filename = \"./drive/MyDrive/output_hm/train_recommenders_v1.tfrecord\"\n",
        "train = tf.data.TFRecordDataset(train_filename)\n",
        "\n",
        "test_filename = \"./drive/MyDrive/output_hm/test_recommenders_v1.tfrecord\"\n",
        "test = tf.data.TFRecordDataset(test_filename)\n",
        "feature_description = {\n",
        "    \"context_article_id\":  tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_product_code\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_product_type_no\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_graphical_appearance_no\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_colour_group_code\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_perceived_colour_value_id\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_perceived_colour_master_id\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_department_no\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_index_code\": tf.io.FixedLenFeature([5], tf.string, default_value=np.repeat(\"Missing\", 5)),\n",
        "    \"context_index_group_no\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_section_no\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_garment_group_no\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_timestamp\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_price\": tf.io.FixedLenFeature([5], tf.float32, default_value=np.repeat(0.0, 5)),\n",
        "    \"context_sales_channel_id\": tf.io.FixedLenFeature([5], tf.int64, default_value=np.repeat(-1, 5)),\n",
        "    \"context_fn\": tf.io.FixedLenFeature([1], tf.int64, default_value=-1),\n",
        "    \"context_active\": tf.io.FixedLenFeature([1], tf.int64, default_value=-1),\n",
        "    \"context_club_member_status\": tf.io.FixedLenFeature([1], tf.string, default_value=\"Missing\"),\n",
        "    \"context_fashion_news_frequency\": tf.io.FixedLenFeature([1], tf.string, default_value=\"Missing\"),\n",
        "    \"context_age\": tf.io.FixedLenFeature([1], tf.int64, default_value=-1),\n",
        "    \"label_article_id\": tf.io.FixedLenFeature([1], tf.int64, default_value=-1),\n",
        "}\n",
        "\n",
        "def _parse_function(example_proto):\n",
        "  return tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "train_ds = train.map(_parse_function).map(lambda x: {\n",
        "  \"article_id\": tf.strings.as_string(x[\"context_article_id\"]),\n",
        "  \"product_code\": x[\"context_product_code\"],\n",
        "  \"product_type_no\": x[\"context_product_type_no\"],\n",
        "  \"graphical_appearance_no\": x[\"context_graphical_appearance_no\"],\n",
        "  \"colour_group_code\": x[\"context_colour_group_code\"],\n",
        "  \"perceived_colour_value_id\": x[\"context_perceived_colour_value_id\"],\n",
        "  \"perceived_colour_master_id\": x[\"context_perceived_colour_master_id\"],\n",
        "  \"department_no\": x[\"context_department_no\"],\n",
        "  \"index_code\": x[\"context_index_code\"],\n",
        "  \"index_group_no\": x[\"context_index_group_no\"],\n",
        "  \"section_no\": x[\"context_section_no\"],\n",
        "  \"garment_group_no\": x[\"context_garment_group_no\"],\n",
        "  \"timestamp\": x[\"context_timestamp\"],\n",
        "  \"price\": x[\"context_price\"],\n",
        "  \"sales_channel_id\": x[\"context_sales_channel_id\"],\n",
        "  \"fn\": x[\"context_fn\"],\n",
        "  \"active\": x[\"context_active\"],\n",
        "  \"club_member_status\": x[\"context_club_member_status\"],\n",
        "  \"fashion_news_frequency\": x[\"context_fashion_news_frequency\"],\n",
        "  \"age\": x[\"context_age\"],\n",
        "  \"label_article_id\": tf.strings.as_string(x[\"label_article_id\"])\n",
        "})\n",
        "\n",
        "test_ds = test.map(_parse_function).map(lambda x: {\n",
        "  \"article_id\": tf.strings.as_string(x[\"context_article_id\"]),\n",
        "  \"product_code\": x[\"context_product_code\"],\n",
        "  \"product_type_no\": x[\"context_product_type_no\"],\n",
        "  \"graphical_appearance_no\": x[\"context_graphical_appearance_no\"],\n",
        "  \"colour_group_code\": x[\"context_colour_group_code\"],\n",
        "  \"perceived_colour_value_id\": x[\"context_perceived_colour_value_id\"],\n",
        "  \"perceived_colour_master_id\": x[\"context_perceived_colour_master_id\"],\n",
        "  \"department_no\": x[\"context_department_no\"],\n",
        "  \"index_code\": x[\"context_index_code\"],\n",
        "  \"index_group_no\": x[\"context_index_group_no\"],\n",
        "  \"section_no\": x[\"context_section_no\"],\n",
        "  \"garment_group_no\": x[\"context_garment_group_no\"],\n",
        "  \"timestamp\": x[\"context_timestamp\"],\n",
        "  \"price\": x[\"context_price\"],\n",
        "  \"sales_channel_id\": x[\"context_sales_channel_id\"],\n",
        "  \"fn\": x[\"context_fn\"],\n",
        "  \"active\": x[\"context_active\"],\n",
        "  \"club_member_status\": x[\"context_club_member_status\"],\n",
        "  \"fashion_news_frequency\": x[\"context_fashion_news_frequency\"],\n",
        "  \"age\": x[\"context_age\"],\n",
        "  \"label_article_id\": tf.strings.as_string(x[\"label_article_id\"])\n",
        "})\n",
        "\n",
        "for x in train_ds.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles = train_ds.map(lambda x: tf.squeeze(x[\"label_article_id\"]))\n",
        "articles"
      ],
      "metadata": {
        "id": "DlniVImsXL-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b3bbae-a108-470e-af41-0ab67bb2039f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str_features_dense = ['fashion_news_frequency', 'club_member_status']\n",
        "int_features_dense = ['fn', 'active']\n",
        "\n",
        "str_features_time = ['index_code']\n",
        "int_features_time = ['section_no', 'sales_channel_id', 'product_type_no', 'product_code', 'perceived_colour_value_id', 'perceived_colour_master_id', 'index_group_no', \n",
        "                     'graphical_appearance_no', 'garment_group_no', 'department_no', 'colour_group_code']"
      ],
      "metadata": {
        "id": "5U1pFFh0AcC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r\"drive/MyDrive/output_hm/stats_dict.pkl\", \"rb\") as stats_file:\n",
        "  stats_dict = pickle.load(stats_file)"
      ],
      "metadata": {
        "id": "CJnj1GH5B3P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CandidateModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding_dimension = 128\n",
        "\n",
        "    self.embedding = {}\n",
        "    vocabulary = tf.strings.as_string(tf.convert_to_tensor(stats_dict['stats_dict']['article_id'])).numpy()\n",
        "    self.embedding['article_id'] = tf.keras.Sequential(\n",
        "        [tf.keras.layers.StringLookup(\n",
        "            vocabulary=vocabulary, mask_token=None),\n",
        "          tf.keras.layers.Embedding(len(vocabulary) + 1,\n",
        "                                    self.embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Compute embeddings for int features.\n",
        "  def call(self, features):\n",
        "    return self.embedding['article_id'](features)    "
      ],
      "metadata": {
        "id": "zqknZ0vX1n-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP3OS_HPeefT"
      },
      "outputs": [],
      "source": [
        "class QueryModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding_dimension = 128\n",
        "\n",
        "    self.str_features_time = ['index_code', 'article_id']\n",
        "    self.int_features_time = ['section_no', 'sales_channel_id', 'product_type_no', 'product_code', 'perceived_colour_value_id', 'perceived_colour_master_id', 'index_group_no', \n",
        "                              'graphical_appearance_no', 'garment_group_no', 'department_no', 'colour_group_code']\n",
        "    self.str_features_dense = ['fashion_news_frequency', 'club_member_status']\n",
        "    self.int_features_dense = ['fn', 'active']\n",
        "\n",
        "    self.time_embedding = {}\n",
        "    self.dense_embedding = {}\n",
        "\n",
        "    for feature_name in self.str_features_time:\n",
        "      if feature_name == 'article_id':\n",
        "        vocabulary=tf.strings.as_string(tf.convert_to_tensor(stats_dict['stats_dict']['article_id'])).numpy()\n",
        "      else:\n",
        "        vocabulary= np.array(stats_dict['stats_dict'][feature_name])\n",
        "      self.time_embedding[feature_name] = tf.keras.Sequential(\n",
        "          [tf.keras.layers.StringLookup(\n",
        "              vocabulary=vocabulary, mask_token=None),\n",
        "           tf.keras.layers.Embedding(len(vocabulary) + 1,\n",
        "                                     self.embedding_dimension),\n",
        "           tf.keras.layers.GRU(self.embedding_dimension, name=f'gru_{feature_name}', return_sequences=False),\n",
        "    ])\n",
        "\n",
        "    # Compute embeddings for int features.\n",
        "    for feature_name in self.int_features_time:\n",
        "      vocabulary = tf.convert_to_tensor(stats_dict['stats_dict'][feature_name])\n",
        "      self.time_embedding[feature_name] = tf.keras.Sequential(\n",
        "          [tf.keras.layers.IntegerLookup(\n",
        "              vocabulary=vocabulary, mask_value=None),\n",
        "           tf.keras.layers.Embedding(len(vocabulary) + 1,\n",
        "                                     self.embedding_dimension),\n",
        "           tf.keras.layers.GRU(self.embedding_dimension, name=f'gru_{feature_name}', return_sequences=False),\n",
        "    ])\n",
        "\n",
        "    \"\"\"\n",
        "    # Compute embeddings for int features.\n",
        "    for feature_name in self.str_features_dense:\n",
        "      vocabulary = tf.convert_to_tensor(stats_dict['stats_dict'][feature_name])\n",
        "      self.dense_embedding[feature_name] = tf.keras.Sequential(\n",
        "          [tf.keras.layers.StringLookup(\n",
        "              vocabulary=vocabulary, mask_token=None),\n",
        "           tf.keras.layers.Embedding(len(vocabulary) + 1,\n",
        "                                     104),\n",
        "    ])\n",
        "\n",
        "    # Compute embeddings for int features.\n",
        "    for feature_name in self.int_features_dense:\n",
        "      vocabulary = tf.convert_to_tensor(stats_dict['stats_dict'][feature_name])\n",
        "      self.dense_embedding[feature_name] = tf.keras.Sequential(\n",
        "          [tf.keras.layers.IntegerLookup(\n",
        "              vocabulary=vocabulary, mask_value=None),\n",
        "           tf.keras.layers.Embedding(len(vocabulary) + 1,\n",
        "                                     104),\n",
        "    ]) \n",
        "    \"\"\"\n",
        "    self._cross_layer_1 = tfrs.layers.dcn.Cross(\n",
        "            projection_dim=self.embedding_dimension,\n",
        "            kernel_initializer=\"glorot_uniform\")\n",
        "    \n",
        "    self._cross_layer_2 = tfrs.layers.dcn.Cross(\n",
        "            projection_dim=self.embedding_dimension,\n",
        "            kernel_initializer=\"glorot_uniform\")\n",
        "\n",
        "    self._deep_layers = [tf.keras.layers.Dense(int(self.embedding_dimension*13), activation=\"tanh\")\n",
        "      for layer_size in range(4)]\n",
        "    self.last_layers = [tf.keras.layers.Dense(self.embedding_dimension, activation='relu') for layer_size in range(2)]\n",
        "    self.last_layers.append(tf.keras.layers.Dense(self.embedding_dimension, activation='tanh'))\n",
        "\n",
        "  def call(self, features):\n",
        "    # Concatenate embeddings\n",
        "    embeddings_time = []\n",
        "    embeddings_dense = []\n",
        "    for feature_name in self.str_features_time:\n",
        "      embedding_fn = self.time_embedding[feature_name]\n",
        "      embeddings_time.append(embedding_fn(features[feature_name]))\n",
        "\n",
        "    for feature_name in self.int_features_time:\n",
        "      embedding_fn = self.time_embedding[feature_name]\n",
        "      embeddings_time.append(embedding_fn(features[feature_name]))\n",
        "    \"\"\"\n",
        "    for feature_name in self.str_features_dense:\n",
        "      embedding_fn = self.dense_embedding[feature_name]\n",
        "      embeddings_dense.append(embedding_fn(features[feature_name]))\n",
        "\n",
        "    for feature_name in self.int_features_dense:\n",
        "      embedding_fn = self.dense_embedding[feature_name]\n",
        "      embeddings_dense.append(embedding_fn(features[feature_name]))\n",
        "    \"\"\"\n",
        "    concated_time = tf.concat(embeddings_time, axis=1)\n",
        "    #concated_dense = tf.concat(embeddings_dense, axis=2)\n",
        "    #print(concated_dense)\n",
        "\n",
        "    #concate = tf.concat([concated_time + concated_dense], axis=1)\n",
        "    x = self._cross_layer_1(concated_time)\n",
        "    x = self._cross_layer_2(x)\n",
        "    #self.timestamp_embedding(features[\"timestamp\"]),\n",
        "    #self.age_embedding(features[\"age\"]),\n",
        "    for deep_layer in self._deep_layers:\n",
        "      concated_time = deep_layer(concated_time)               \n",
        "    # Build Cross Network\n",
        "    #if self._cross_layer is not None:\n",
        "    #  x0 = self._cross_layer(x)\n",
        "    x = tf.concat([concated_time, x], axis=1)\n",
        "    for deep_layer in self.last_layers:\n",
        "      x = deep_layer(x)   \n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KDYuhG9u9cq"
      },
      "outputs": [],
      "source": [
        "class CombinedModel(tfrs.models.Model):\n",
        "  def __init__(self, query_model, candidate_model):\n",
        "    super().__init__()    \n",
        "    self.query_model = tf.keras.Sequential([\n",
        "      query_model,\n",
        "      tf.keras.layers.Dense(64, activation='tanh')\n",
        "    ])\n",
        "    self.candidate_model = tf.keras.Sequential([\n",
        "      candidate_model,\n",
        "      tf.keras.layers.Dense(64, activation='tanh')\n",
        "    ])\n",
        "    self.task = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=articles.batch(2048).map(self.candidate_model),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    # We only pass the user id and timestamp features into the query model. This\n",
        "    # is to ensure that the training inputs would have the same keys as the\n",
        "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
        "    # error when loading the query model after saving it.\n",
        "    query_embeddings = self.query_model({\n",
        "      \"article_id\": features[\"article_id\"],\n",
        "      \"product_code\": features[\"product_code\"],\n",
        "      \"product_type_no\": features[\"product_type_no\"],\n",
        "      \"graphical_appearance_no\": features[\"graphical_appearance_no\"],\n",
        "      \"colour_group_code\": features[\"colour_group_code\"],\n",
        "      \"perceived_colour_value_id\": features[\"perceived_colour_value_id\"],\n",
        "      \"perceived_colour_master_id\": features[\"perceived_colour_master_id\"],\n",
        "      \"department_no\": features[\"department_no\"],\n",
        "      \"index_code\": features[\"index_code\"],\n",
        "      \"index_group_no\": features[\"index_group_no\"],\n",
        "      \"section_no\": features[\"section_no\"],\n",
        "      \"garment_group_no\": features[\"garment_group_no\"],\n",
        "      \"sales_channel_id\": features[\"sales_channel_id\"],\n",
        "      'fashion_news_frequency': features[\"fashion_news_frequency\"], \n",
        "      'club_member_status': features[\"club_member_status\"], \n",
        "      'fn': features[\"fn\"], \n",
        "      'active': features[\"active\"],\n",
        "      })\n",
        "    \n",
        "    candidate_embeddings = self.candidate_model(features['label_article_id'])\n",
        "    return self.task(\n",
        "        query_embeddings, candidate_embeddings, compute_metrics=not training)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_model = QueryModel()\n",
        "candidate_model = CandidateModel()"
      ],
      "metadata": {
        "id": "X1OQ8Jr6jepB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af9c572-d14c-495a-9076-98fa4a81ea92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:mask_value is deprecated, use mask_token instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c2wAqPVXYgz"
      },
      "outputs": [],
      "source": [
        "model = CombinedModel(query_model, candidate_model)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.01))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wto55LJWuj0"
      },
      "outputs": [],
      "source": [
        "cached_train = train_ds.shuffle(5_000).batch(1024).cache()\n",
        "cached_test = test_ds.batch(1024).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcXH5aDcWyJJ",
        "outputId": "35fac7a4-4d60-4069-a069-3cae14aef74d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'article_id': <tf.Tensor 'IteratorGetNext:2' shape=(None, 5) dtype=string>, 'product_code': <tf.Tensor 'IteratorGetNext:16' shape=(None, 5) dtype=int64>, 'product_type_no': <tf.Tensor 'IteratorGetNext:17' shape=(None, 5) dtype=int64>, 'graphical_appearance_no': <tf.Tensor 'IteratorGetNext:9' shape=(None, 5) dtype=int64>, 'colour_group_code': <tf.Tensor 'IteratorGetNext:4' shape=(None, 5) dtype=int64>, 'perceived_colour_value_id': <tf.Tensor 'IteratorGetNext:14' shape=(None, 5) dtype=int64>, 'perceived_colour_master_id': <tf.Tensor 'IteratorGetNext:13' shape=(None, 5) dtype=int64>, 'department_no': <tf.Tensor 'IteratorGetNext:5' shape=(None, 5) dtype=int64>, 'index_code': <tf.Tensor 'IteratorGetNext:10' shape=(None, 5) dtype=string>, 'index_group_no': <tf.Tensor 'IteratorGetNext:11' shape=(None, 5) dtype=int64>, 'section_no': <tf.Tensor 'IteratorGetNext:19' shape=(None, 5) dtype=int64>, 'garment_group_no': <tf.Tensor 'IteratorGetNext:8' shape=(None, 5) dtype=int64>, 'sales_channel_id': <tf.Tensor 'IteratorGetNext:18' shape=(None, 5) dtype=int64>, 'fashion_news_frequency': <tf.Tensor 'IteratorGetNext:6' shape=(None, 1) dtype=string>, 'club_member_status': <tf.Tensor 'IteratorGetNext:3' shape=(None, 1) dtype=string>, 'fn': <tf.Tensor 'IteratorGetNext:7' shape=(None, 1) dtype=int64>, 'active': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=int64>}. Consider rewriting this model with the Functional API.\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'article_id': <tf.Tensor 'IteratorGetNext:2' shape=(None, 5) dtype=string>, 'product_code': <tf.Tensor 'IteratorGetNext:16' shape=(None, 5) dtype=int64>, 'product_type_no': <tf.Tensor 'IteratorGetNext:17' shape=(None, 5) dtype=int64>, 'graphical_appearance_no': <tf.Tensor 'IteratorGetNext:9' shape=(None, 5) dtype=int64>, 'colour_group_code': <tf.Tensor 'IteratorGetNext:4' shape=(None, 5) dtype=int64>, 'perceived_colour_value_id': <tf.Tensor 'IteratorGetNext:14' shape=(None, 5) dtype=int64>, 'perceived_colour_master_id': <tf.Tensor 'IteratorGetNext:13' shape=(None, 5) dtype=int64>, 'department_no': <tf.Tensor 'IteratorGetNext:5' shape=(None, 5) dtype=int64>, 'index_code': <tf.Tensor 'IteratorGetNext:10' shape=(None, 5) dtype=string>, 'index_group_no': <tf.Tensor 'IteratorGetNext:11' shape=(None, 5) dtype=int64>, 'section_no': <tf.Tensor 'IteratorGetNext:19' shape=(None, 5) dtype=int64>, 'garment_group_no': <tf.Tensor 'IteratorGetNext:8' shape=(None, 5) dtype=int64>, 'sales_channel_id': <tf.Tensor 'IteratorGetNext:18' shape=(None, 5) dtype=int64>, 'fashion_news_frequency': <tf.Tensor 'IteratorGetNext:6' shape=(None, 1) dtype=string>, 'club_member_status': <tf.Tensor 'IteratorGetNext:3' shape=(None, 1) dtype=string>, 'fn': <tf.Tensor 'IteratorGetNext:7' shape=(None, 1) dtype=int64>, 'active': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=int64>}. Consider rewriting this model with the Functional API.\n",
            "1836/1836 [==============================] - 588s 303ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 6730.8008 - regularization_loss: 0.0000e+00 - total_loss: 6730.8008\n",
            "Epoch 2/20\n",
            "1836/1836 [==============================] - 272s 148ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 6239.6679 - regularization_loss: 0.0000e+00 - total_loss: 6239.6679\n",
            "Epoch 3/20\n",
            "1653/1836 [==========================>...] - ETA: 27s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 6011.2397 - regularization_loss: 0.0000e+00 - total_loss: 6011.2397"
          ]
        }
      ],
      "source": [
        "history = model.fit(cached_train, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(cached_test)"
      ],
      "metadata": {
        "id": "GScO7AIu6aiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0CAUPQA26Vu5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "recommender_model.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}